---
title: "1, scraper"
output: html_document
---
# Підготовка до роботи
Сам R має вбудовані функції: зробити таблицю з даними `data.frame`, виконати арифметичні операції, підрахувати базові статистики, фільтрувати…  Проте багато завдань потребують додаткових функцій. Їх і дають бібліотеки. Ми встанивимо:
- rvest для роботи з html  
- tidyverse — набір пакетів для роботи з даними від Хедлі Вікхема. Tidy має набагато приємніший інтерфейс, ніж базовий R, а також купу допоміжних функцій  


## Markdown
Робочі ноутбуки мають розширення ".Rmd" — це маркдаун в R. Він дозволяє комбінувати текст, код, результат виконання коду в одному файлі. Ноутбуки можна перетворити в інші формати, наприклад, у веб-сторінку або pdf. Більше тут: https://rmarkdown.rstudio.com/lesson-2.html,  https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet

```{r message=FALSE, warning=FALSE, include=FALSE}
install.packages('rvest')    # install 'rvest' library in R; library and package are synonyms
install.packages('tidyverse')
install.packages("progress")
```

У робочому середовищі або скрипті треба імпортувати необхідні бібліотеки:
```{r setup, include=FALSE}
library(rvest)    # a library for web web scraping
library(tidyverse)
library(progress)
```

# Скрейпинг
## Cheatsheets

About HTML: https://www.w3schools.com/html/default.asp
CSS-selectors: https://www.w3schools.com/cssref/css_selectors.asp

### Tidyverse code: piping

`data %>% function1() %>% function2()` - is a **pipe**  
`data` — is our data structure, most often a *DataFrame*  
`function1`, `function1` — are functions, applied to data. The order of them matters!  

1. `data %>% function1()` - `data` is transformed by `function1`.  
2. `data %>% function1() %>% function2()` — data, transformed by `function1`, e.g. the result of `function1`, goes to `function2`.  

The same can be written as:
`data_after_f1 <- function1(data)`  
`data_after_f2 <- function1(data_after_f1)` — much less elegant and clear code, but does the same.  

**You can stack as much functions in pipe as you want!**  

## Let's code!
Заскрейпимо сайт euvsdisinfo.eu, щоб проаналізувати, яка там дезінформація

0. Отримаємо html
```{r}
url <- "https://euvsdisinfo.eu/disinformation-cases/"
content <- read_html(url)
content
```

1. Знайдемо потрібні елементи на сторінці
![find element](1_find-element.png)

```{r}
header <- content %>%
  html_node('div.disinfo-db-columns') %>%
  html_children() %>%
  html_text() %>%
  str_trim()

header

rows <- content %>%
  html_nodes('div.disinfo-db-post')

rows %>%
  html_nodes('div.disinfo-db-cell[data-column]') %>%
  html_text() %>%
  str_trim()
```
### Які проблеми у нас з'явились?
1. Результати представлені в сторінках. Отже треба заходити на кожну сторінку окремо і парсити її. Сторінки однакові
Ми *не*хочемо робити це вручну кілька сот разів!

Рішення — *цикл*
Приклад циклу:
```{r}
iterable <- c(1:15)
iterable

result <- c()

for (item in iterable) {   # назва "item" довільна
  result <- c(result, item ^ 3)
}

result
```
### Цикл для сторінок
Дуже часто номер сторінки просто пишуть в url. У нашому випадку там записаний офсет — скільки рядків відступити. На сторінці видно, що найбільший офсет складає 7300, а на 1 сторінці 10 записів.  
Тобто нам треба пройти циклом по офсетам: 0, 10, 20, 30, …, 7290, 7300.
```{r}
offsets <- seq(0, 500, by=10)    # Послідовність з кроком 10

date <- c()    # Тут будемо зберігати оброблені дані
title <- c()
outlets <- c()
country <- c()
link <- c()

pb <- progress_bar$new(total = length(offsets))    # щоб бачити, як просувається скрейпінг

for (o in offsets) {
  url <- sprintf("https://euvsdisinfo.eu/disinformation-cases/?offset=%s", o)    # Вставляємо офсет в url
  rows <- read_html(url) %>%
    html_nodes('div.disinfo-db-post')    # Копі-паст коду для першої сторінки
  
  # візьмемо всі клітинки, щоб не повторювати цей код 4 рази
  cells <- rows %>%
    html_nodes("div.disinfo-db-cell[data-column]") %>%
    html_text() %>%
    str_trim()    # це видаляє зайві пробіли на початку і в кінці рядка
  
  # додамо результати: дати до дат, назви до назв і т.д.
  # seq(1, 40, 4) дасть вектор c(1, 5, 9, 13 …). Поставивши його в квадратні дужки, отримаємо перше, п'яте, дев'яте і т.д. значення у векторі.
  date    <- c(date,    cells[seq(1, length(cells), 4)])
  title   <- c(title,   cells[seq(2, length(cells), 4)])
  outlets <- c(outlets, cells[seq(3, length(cells), 4)])
  country <- c(country, cells[seq(4, length(cells), 4)])
  
  # лінки окремо, адже там інший css-селектор
  links_in_loop <- rows %>%
    html_nodes(".cell-title a") %>%
    html_attr("href")
  
  link <- c(link, links_in_loop)
  
  pb$tick()
  
  Sys.sleep(2)    # відсилати запит кожні 2 секунди, а не безперервно. Щоб не нашкодити сайту
}
```

### Таблиця з результатами
```{r}
df <- data.frame(date = date,
                 title = title,
                 outlets = outlets,
                 country = country,
                 link = link)

df
```


## Збереження даних
```{r}
write.csv(df, 'eu_vs_disinfo.csv')
```

```{r}
library(lubridate)
df <- df %>%
  mutate(covid19 = str_detect(title, "[Cc]ovid.?19|[Cc]oronavirus"),
         date = dmy(df$date))

filter(df, date > "2020-02-01")

ggplot(filter(df, date > "2020-02-01"), aes(date, fill = covid19)) + 
  geom_dotplot(color = NA, binwidth = 1, dotsize = 0.75, stackgroups = TRUE, method = "histodot") +
  labs(title = "Кількість дезінформаійних новин", caption = "Дані: euvsdisinfo.eu") + 
  theme_light() +
  theme(panel.border = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        plot.margin = margin(24, 24, 24, 16))
```


