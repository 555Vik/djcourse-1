---
title: "1, scraper"
output: html_document
---
# Підготовка до роботи
Сам R має вбудовані функції: зробити таблицю з даними `data.frame`, виконати арифметичні операції, підрахувати базові статистики, фільтрувати…  Проте багато завдань потребують додаткових функцій. Їх і дають бібліотеки. Ми встанивимо:
- rvest для роботи з html  
- tidyverse — набір пакетів для роботи з даними від Хедлі Вікхема. Tidy має набагато приємніший інтерфейс, ніж базовий R, а також купу допоміжних функцій  


## Markdown
Робочі ноутбуки мають розширення ".Rmd" — це маркдаун в R. Він дозволяє комбінувати текст, код, результат виконання коду в одному файлі. Ноутбуки можна перетворити в інші формати, наприклад, у веб-сторінку або pdf. Більше тут: https://rmarkdown.rstudio.com/lesson-2.html,  https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet

```{r message=FALSE, warning=FALSE, include=FALSE}
install.packages('rvest')    # install 'rvest' library in R; library and package are synonyms
install.packages('tidyverse')
install.packages("progress")
```

У робочому середовищі або скрипті треба імпортувати необхідні бібліотеки:
```{r setup, include=FALSE}
library(rvest)    # a library for web web scraping
library(tidyverse)
library(progress)
```

# Скрейпинг
## Cheatsheets

About HTML: https://www.w3schools.com/html/default.asp
CSS-selectors: https://www.w3schools.com/cssref/css_selectors.asp

### Tidyverse code: piping

`data %>% function1() %>% function2()` - is a **pipe**  
`data` — is our data structure, most often a *DataFrame*  
`function1`, `function1` — are functions, applied to data. The order of them matters!  

1. `data %>% function1()` - `data` is transformed by `function1`.  
2. `data %>% function1() %>% function2()` — data, transformed by `function1`, e.g. the result of `function1`, goes to `function2`.  

The same can be written as:
`data_after_f1 <- function1(data)`  
`data_after_f2 <- function1(data_after_f1)` — much less elegant and clear code, but does the same.  

**You can stack as much functions in pipe as you want!**  

## Let's code!
Заскрейпимо сайт euvsdisinfo.eu, щоб проаналізувати, яка там дезінформація

0. Отримаємо html
```{r}
url <- "https://euvsdisinfo.eu/disinformation-cases/"
content <- read_html(url)
content
```

1. Знайдемо потрібні елементи на сторінці (клік правою кнопкою миші на елементі => "Перевірити"(або "Inspect")б щось подібне), ctrl+shift+c на сторінці.
![find element](1_find-element.png)

Спробуємо вибрати рядки з таблиці
```{r}
content %>%
  html_nodes("div.disinfo-db-post") %>%
  html_text()

# <div class="disinfo-db-post> "
```
Поки не дуже гарно, весь зміст рядка "злипся", інформація не структурована так, як ми хочемо.  

Виберемо лише заголовки та дати, використаємо для цього CSS-селектори за допомогою атрибутів.
```{r}
titles <- content %>%
  html_nodes('div.disinfo-db-post [data-column="Title"]') %>%
  html_text() %>%
  str_trim()

dates <- content %>%
  html_nodes('div.disinfo-db-post [data-column="Date"]') %>%
  html_text() %>%
  str_trim()

dates
```
Селектор `'div.disinfo-db-post [data-column="Title"]'` означає: вибрати всі елементи, у яких атрибут `data-column` дорівнює `Title`, і які знаходяться всередині елемента `div` з класом `disinfo-db-post`, це визначено через пробіл у селекторі. Ми тут трохи ускладнили з практичного, зробивши ієрархічний селектор.

Супер, маємо дані! Пора зробити з них таблицю — `data.frame` — і зберегти її. 
```{r}
df <- data.frame(titles = titles, date = dates)
# синтаксис: data.frame(назва_колонки = назва_вектора_значень, ще_одна_колонка=…)

write.csv(df, "eu_disinfo_1st_page.csv", row.names = FALSE)    # записали дані в форматі .csv
# Файл з даними буде у тій же папці, де збережено ноутбук
```

Спробуйте відкрити файл у текстовому редакторі (Блокнот тощо, хороший на віндоус Notepad++, для маків та лінуксів Sublime Text, чи в самому RStudio).
CSV — рядки відділені новими лініями, значення комірок розділені комами:
```
"","titles","date"
"1","The COVID-19 outbreak is a pretext to gain control of the world’s population","25.03.2020"
"2","Swedish oligarchs impede the introduction of quarantine measures","24.03.2020"
```
(вище просто текстовий чанк, він не виконається як код, але виділиться візуально)

Як прочитати csv
```{r}
read.csv("eu_disinfo_1st_page.csv")
```


## Зациклюємо. Скачаємо те саме для кожної сторінки
Адреса сайту  виглядає так: https://euvsdisinfo.eu/disinformation-cases/?offset=20  
`offset=20` це друга сторінка, у третьої оффсет буде 30, у одинадцятої 110 і так далі.
Отже щоб завантажити дані з однієї сторінки потрібно:
1. Завантажити її html, підставляючи по ходу цикла різні офсети
2. Вибрати потрібні нам дані з відповідних елементів (як ми вибрали дати та заголовки з першої сторінки)
3. Зберегти дані

```{r}
pb <- progress_bar$new(
  format = "  downloading [:bar] :percent in :elapsed ",
  total = 50, clear = FALSE, width= 60)
# скопіювала з довідки про progress_bar, змінено лише параметр total

# Вектори, у яких будемо зберігати значення
dates <- c()
titles <- c()
links <- c()

url_template <- "https://euvsdisinfo.eu/disinformation-cases/?offset="
```

Візьмемо перші 50 сторінок, всередині цикла те саме, що ми робили з першою сторінкою
```{r}
for (page in 1:50) {
  # з'єднуємо рядки: основу url-адреси, №сторінки помножений на 10, бо сторінки йдуть з кроком 10
  url <- str_c(url_template,
               page * 10)
  
  content <- read_html(url)
  
  # Копіпаст коду для першої сторінки
  titles <- content %>%
    html_nodes('div.disinfo-db-post [data-column="Title"]') %>%
    html_text() %>%
    str_trim() %>%
    c(titles, .)    # "." крапка означає змінну в пайпі, якщо пона не на першому місці 
  
  dates <- content %>%
    html_nodes('div.disinfo-db-post [data-column="Date"]') %>%
    html_text() %>%
    str_trim()  %>%
    c(dates, .)
  
  # ще додамо лінки. Тут вибираємо не текст, а атрибут "href" тега "<a>" — лінк
  links <- content %>%
    html_nodes('div.disinfo-db-post [data-column="Title"] a') %>%
    html_attr("href") %>%
    c(links, .)
  
  # Ще один важливий крок: затримка між запитами, щоб не зробити DDoS-атаку на сайт
  Sys.sleep(2)    # 2 секунди програма буде "спати" 
  
  # Оновимо прогрес-бар. Це для комфорту, щоб бачити, скільки ще лишилось сторінок
  pb$tick()
}
```

Перевіримо, вектори мають мати довжиноу 500 при офсеті 50. Далі робимо з них датафрейм і зберігаємо його
```{r}
stopifnot(length(dates) == 500 &
          length(titles) == 500 &
          length(links) == 500)
# Якщо довжина кожного з векторів не 500, нам виб'є помилку. Ми просимо R через `stopifnot` вважати помилкою, якщо в результаті скрейпінгу у нас не збігається кільність елементів у колонках.

# датафрейм через пайп одразу йде на зберігання
data.frame(title = titles,
           date = dates,
           link = links) %>%
  write.csv("eudisinfo_all_pages.csv",
            row.names = FALSE) # щоб не зберігати непотрібну колонку номерів рядків
```

Прочитаємо датафрейм, який щойно зберегли:
```{r}
df <- read.csv("eudisinfo_all_pages.csv")
df
```
Відскрейпили.

## Невелика візуалізація

```{r}
library(lubridate)

df <- df %>%
  mutate(covid19 = str_detect(title, "[Cc]ovid.?19|[Cc]oronavirus"),
         date = dmy(df$date))

filter(df, date > "2020-02-01")

ggplot(filter(df, date > "2020-02-01"), aes(date, fill = covid19)) + 
  geom_dotplot(color = NA, binwidth = 1, dotsize = 0.75, stackgroups = TRUE, method = "histodot") +
  labs(title = "Кількість дезінформаійних новин", caption = "Дані: euvsdisinfo.eu") + 
  theme_light() +
  theme(panel.border = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        plot.margin = margin(24, 24, 24, 16))
```


